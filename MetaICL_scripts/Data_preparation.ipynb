{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7163cf71-f2c3-4aae-9ba5-65f496ae3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "def read_jsonl_file(filepath):\n",
    "    \"\"\"Read a JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return [json.loads(line.strip()) for line in file if line.strip()]\n",
    "\n",
    "def record_tasks_and_jsonl(root_folder):\n",
    "    \"\"\"Record task names and corresponding JSONL file contents.\"\"\"\n",
    "    tasks_dict = {}\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for directory in dirs:\n",
    "            task_path = os.path.join(root, directory)\n",
    "            jsonl_files = [file for file in os.listdir(task_path) if file.endswith('.jsonl')]\n",
    "            task_data = []\n",
    "            for jsonl_file in jsonl_files:\n",
    "                file_path = os.path.join(task_path, jsonl_file)\n",
    "                task_data.extend(read_jsonl_file(file_path))\n",
    "            tasks_dict[directory] = task_data\n",
    "    return tasks_dict\n",
    "\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    \"\"\"Split data into training and testing sets based on the specified ratio.\"\"\"\n",
    "    random.shuffle(data)  # Shuffle the data to ensure randomness\n",
    "    split_point = int(len(data) * train_ratio)\n",
    "    return data[:split_point], data[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33faee8d-1490-497c-80f9-e5eeaf0edbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current directory as the root folder\n",
    "root_folder = os.getcwd()\n",
    "tasks_info = record_tasks_and_jsonl(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fc834c-6054-4ab0-89a3-3c60e08e50d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: ethos-national_origin\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: glue-cola\n",
      "Training set size: 4300\n",
      "Testing set size: 1075\n",
      "\n",
      "Task: anli\n",
      "Training set size: 4128\n",
      "Testing set size: 1032\n",
      "\n",
      "Task: lama-google_re\n",
      "Training set size: 5016\n",
      "Testing set size: 1254\n",
      "\n",
      "Task: yelp_polarity\n",
      "Training set size: 30528\n",
      "Testing set size: 7632\n",
      "\n",
      "Task: rotten_tomatoes\n",
      "Training set size: 4392\n",
      "Testing set size: 1098\n",
      "\n",
      "Task: blimp-anaphor_number_agreement\n",
      "Training set size: 928\n",
      "Testing set size: 232\n",
      "\n",
      "Task: sick\n",
      "Training set size: 2108\n",
      "Testing set size: 527\n",
      "\n",
      "Task: tweet_eval-irony\n",
      "Training set size: 3948\n",
      "Testing set size: 987\n",
      "\n",
      "Task: wino_grande\n",
      "Training set size: 5196\n",
      "Testing set size: 1299\n",
      "\n",
      "Task: glue-sst2\n",
      "Training set size: 3616\n",
      "Testing set size: 904\n",
      "\n",
      "Task: sciq\n",
      "Training set size: 3676\n",
      "Testing set size: 919\n",
      "\n",
      "Task: trec\n",
      "Training set size: 4492\n",
      "Testing set size: 1123\n",
      "\n",
      "Task: health_fact\n",
      "Training set size: 4984\n",
      "Testing set size: 1246\n",
      "\n",
      "Task: superglue-rte\n",
      "Training set size: 1236\n",
      "Testing set size: 309\n",
      "\n",
      "Task: race-middle\n",
      "Training set size: 5872\n",
      "Testing set size: 1468\n",
      "\n",
      "Task: wiki_qa\n",
      "Training set size: 11060\n",
      "Testing set size: 2765\n",
      "\n",
      "Task: scitail\n",
      "Training set size: 5344\n",
      "Testing set size: 1336\n",
      "\n",
      "Task: proto_qa\n",
      "Training set size: 14181\n",
      "Testing set size: 3546\n",
      "\n",
      "Task: crows_pairs\n",
      "Training set size: 1336\n",
      "Testing set size: 334\n",
      "\n",
      "Task: race-high\n",
      "Training set size: 13932\n",
      "Testing set size: 3483\n",
      "\n",
      "Task: lama-trex\n",
      "Training set size: 52304\n",
      "Testing set size: 13076\n",
      "\n",
      "Task: kilt_hotpotqa\n",
      "Training set size: 22528\n",
      "Testing set size: 5632\n",
      "\n",
      "Task: lama-conceptnet\n",
      "Training set size: 23948\n",
      "Testing set size: 5987\n",
      "\n",
      "Task: quartz-with_knowledge\n",
      "Training set size: 1664\n",
      "Testing set size: 416\n",
      "\n",
      "Task: superglue-cb\n",
      "Training set size: 352\n",
      "Testing set size: 88\n",
      "\n",
      "Task: tweet_eval-stance_hillary\n",
      "Training set size: 404\n",
      "Testing set size: 101\n",
      "\n",
      "Task: tweet_eval-offensive\n",
      "Training set size: 5424\n",
      "Testing set size: 1356\n",
      "\n",
      "Task: sms_spam\n",
      "Training set size: 4588\n",
      "Testing set size: 1147\n",
      "\n",
      "Task: kilt_ay2\n",
      "Training set size: 19264\n",
      "Testing set size: 4816\n",
      "\n",
      "Task: glue-qqp\n",
      "Training set size: 161848\n",
      "Testing set size: 40462\n",
      "\n",
      "Task: tab_fact\n",
      "Training set size: 51296\n",
      "Testing set size: 12824\n",
      "\n",
      "Task: art\n",
      "Training set size: 6256\n",
      "Testing set size: 1564\n",
      "\n",
      "Task: paws\n",
      "Training set size: 32128\n",
      "Testing set size: 8032\n",
      "\n",
      "Task: glue-wnli\n",
      "Training set size: 412\n",
      "Testing set size: 103\n",
      "\n",
      "Task: piqa\n",
      "Training set size: 7480\n",
      "Testing set size: 1870\n",
      "\n",
      "Task: ropes\n",
      "Training set size: 6880\n",
      "Testing set size: 1720\n",
      "\n",
      "Task: squad-with_context\n",
      "Training set size: 42408\n",
      "Testing set size: 10602\n",
      "\n",
      "Task: discovery\n",
      "Training set size: 34928\n",
      "Testing set size: 8732\n",
      "\n",
      "Task: superglue-wic\n",
      "Training set size: 2680\n",
      "Testing set size: 670\n",
      "\n",
      "Task: hotpot_qa\n",
      "Training set size: 29748\n",
      "Testing set size: 7437\n",
      "\n",
      "Task: tweet_eval-sentiment\n",
      "Training set size: 8128\n",
      "Testing set size: 2032\n",
      "\n",
      "Task: ethos-religion\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: wikisql\n",
      "Training set size: 33812\n",
      "Testing set size: 8453\n",
      "\n",
      "Task: squad-no_context\n",
      "Training set size: 42408\n",
      "Testing set size: 10602\n",
      "\n",
      "Task: glue-mrpc\n",
      "Training set size: 1760\n",
      "Testing set size: 440\n",
      "\n",
      "Task: ai2_arc\n",
      "Training set size: 1324\n",
      "Testing set size: 331\n",
      "\n",
      "Task: quarel\n",
      "Training set size: 1240\n",
      "Testing set size: 310\n",
      "\n",
      "Task: xsum\n",
      "Training set size: 45456\n",
      "Testing set size: 11364\n",
      "\n",
      "Task: ethos-race\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: glue-rte\n",
      "Training set size: 1236\n",
      "Testing set size: 309\n",
      "\n",
      "Task: glue-mnli\n",
      "Training set size: 39388\n",
      "Testing set size: 9847\n",
      "\n",
      "Task: liar\n",
      "Training set size: 5264\n",
      "Testing set size: 1316\n",
      "\n",
      "Task: tweet_eval-stance_climate\n",
      "Training set size: 288\n",
      "Testing set size: 72\n",
      "\n",
      "Task: onestop_english\n",
      "Training set size: 584\n",
      "Testing set size: 146\n",
      "\n",
      "Task: poem_sentiment\n",
      "Training set size: 548\n",
      "Testing set size: 137\n",
      "\n",
      "Task: boolq\n",
      "Training set size: 13208\n",
      "Testing set size: 3302\n",
      "\n",
      "Task: openbookqa\n",
      "Training set size: 2128\n",
      "Testing set size: 532\n",
      "\n",
      "Task: hate_speech_offensive\n",
      "Training set size: 19956\n",
      "Testing set size: 4989\n",
      "\n",
      "Task: kilt_nq\n",
      "Training set size: 11476\n",
      "Testing set size: 2869\n",
      "\n",
      "Task: social_i_qa\n",
      "Training set size: 7944\n",
      "Testing set size: 1986\n",
      "\n",
      "Task: web_questions\n",
      "Training set size: 3152\n",
      "Testing set size: 788\n",
      "\n",
      "Task: tweet_eval-hate\n",
      "Training set size: 4124\n",
      "Testing set size: 1031\n",
      "\n",
      "Task: medical_questions_pairs\n",
      "Training set size: 2568\n",
      "Testing set size: 642\n",
      "\n",
      "Task: mc_taco\n",
      "Training set size: 3156\n",
      "Testing set size: 789\n",
      "\n",
      "Task: glue-qnli\n",
      "Training set size: 21980\n",
      "Testing set size: 5495\n",
      "\n",
      "Task: ag_news\n",
      "Training set size: 30528\n",
      "Testing set size: 7632\n",
      "\n",
      "Task: qasc\n",
      "Training set size: 3832\n",
      "Testing set size: 958\n",
      "\n",
      "Task: tweet_eval-emotion\n",
      "Training set size: 1624\n",
      "Testing set size: 406\n",
      "\n",
      "Task: ethos-sexual_orientation\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: quoref\n",
      "Training set size: 9800\n",
      "Testing set size: 2450\n",
      "\n",
      "Task: wiqa\n",
      "Training set size: 27704\n",
      "Testing set size: 6926\n",
      "\n",
      "Task: circa\n",
      "Training set size: 26928\n",
      "Testing set size: 6732\n",
      "\n",
      "Task: imdb\n",
      "Training set size: 100128\n",
      "Testing set size: 25032\n",
      "\n",
      "Task: hatexplain\n",
      "Training set size: 7816\n",
      "Testing set size: 1954\n",
      "\n",
      "Task: superglue-wsc\n",
      "Training set size: 544\n",
      "Testing set size: 136\n",
      "\n",
      "Task: kilt_fever\n",
      "Training set size: 41904\n",
      "Testing set size: 10476\n",
      "\n",
      "Task: codah\n",
      "Training set size: 2352\n",
      "Testing set size: 588\n",
      "\n",
      "Task: .ipynb_checkpoints\n",
      "Training set size: 0\n",
      "Testing set size: 0\n",
      "\n",
      "Task: superglue-record\n",
      "Training set size: 40128\n",
      "Testing set size: 10032\n",
      "\n",
      "Task: blimp-ellipsis_n_bar_2\n",
      "Training set size: 928\n",
      "Testing set size: 232\n",
      "\n",
      "Task: ethos-gender\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: ade_corpus_v2-dosage\n",
      "Training set size: 352\n",
      "Testing set size: 88\n",
      "\n",
      "Task: ethos-disability\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: google_wellformed_query\n",
      "Training set size: 13316\n",
      "Testing set size: 3329\n",
      "\n",
      "Task: tweet_eval-stance_atheism\n",
      "Training set size: 336\n",
      "Testing set size: 84\n",
      "\n",
      "Task: hellaswag\n",
      "Training set size: 40296\n",
      "Testing set size: 10074\n",
      "\n",
      "Task: kilt_zsre\n",
      "Training set size: 15024\n",
      "Testing set size: 3756\n",
      "\n",
      "Task: blimp-sentential_negation_npi_licensor_present\n",
      "Training set size: 928\n",
      "Testing set size: 232\n",
      "\n",
      "Task: climate_fever\n",
      "Training set size: 1356\n",
      "Testing set size: 339\n",
      "\n",
      "Task: quartz-no_knowledge\n",
      "Training set size: 1664\n",
      "Testing set size: 416\n",
      "\n",
      "Task: tweet_eval-stance_feminist\n",
      "Training set size: 396\n",
      "Testing set size: 99\n",
      "\n",
      "Task: superglue-multirc\n",
      "Training set size: 3927\n",
      "Testing set size: 982\n",
      "\n",
      "Task: kilt_trex\n",
      "Training set size: 20128\n",
      "Testing set size: 5032\n",
      "\n",
      "Task: superglue-copa\n",
      "Training set size: 528\n",
      "Testing set size: 132\n",
      "\n",
      "Task: qa_srl\n",
      "Training set size: 8860\n",
      "Testing set size: 2215\n",
      "\n",
      "Task: quail\n",
      "Training set size: 8784\n",
      "Testing set size: 2196\n",
      "\n",
      "Task: tweet_eval-stance_abortion\n",
      "Training set size: 392\n",
      "Testing set size: 98\n",
      "\n",
      "Task: ethos-directed_vs_generalized\n",
      "Training set size: 476\n",
      "Testing set size: 119\n",
      "\n",
      "Task: blimp-sentential_negation_npi_scope\n",
      "Training set size: 928\n",
      "Testing set size: 232\n",
      "\n",
      "Task: lama-squad\n",
      "Training set size: 372\n",
      "Testing set size: 93\n",
      "\n",
      "Task: tweet_eval-emoji\n",
      "Training set size: 20128\n",
      "Testing set size: 5032\n",
      "\n",
      "Task: commonsense_qa\n",
      "Training set size: 5012\n",
      "Testing set size: 1253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the train-test split for each task\n",
    "train_test_split = {}\n",
    "for task, data in tasks_info.items():\n",
    "    train, test = split_data(data)\n",
    "    train_test_split[task] = {'train': train, 'test': test}\n",
    "\n",
    "# Print summary information about the splits\n",
    "for task, splits in train_test_split.items():\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"Training set size: {len(splits['train'])}\")\n",
    "    print(f\"Testing set size: {len(splits['test'])}\")\n",
    "    print()  # Print a newline for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88372a7-15b3-4522-81ab-b35756fb6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task_data_splits.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(train_test_split, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172f862e-5ca8-4751-9375-68b82cd76deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "with open('task_data_splits.json', 'r', encoding='utf-8') as json_file:\n",
    "    train_test_split = json.load(json_file)\n",
    "selected_task_names=['superglue-cb', 'tweet_eval-stance_hillary', 'ethos-national_origin', 'blimp-anaphor_number_agreement', 'superglue-rte', 'crows_pairs', 'quartz-with_knowledge', 'sick', 'glue-sst2', 'sciq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a20eee4-9ecd-4d1d-8537-02d53433aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embeddings\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "  api_key=\"\", \n",
    ")\n",
    "def create_embedding(text):\n",
    "    try:\n",
    "        embedding = client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=text,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "        return embedding.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error in embedding creation: {e[:50]}\")\n",
    "        return [] \n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {task: {'train': [], 'test': []} for task in selected_task_names}\n",
    "\n",
    "# Iterate over selected tasks and process each entry\n",
    "for task in selected_task_names:\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        for entry in train_test_split[task][dataset_type]:\n",
    "            combined_text = f\"{entry['input']} {entry['output']}\"\n",
    "            embedding = create_embedding(combined_text)\n",
    "            results[task][dataset_type].append({\n",
    "                'input': entry['input'],\n",
    "                'output': entry['output'],\n",
    "                'options': entry['options'],\n",
    "                'combined_text': combined_text,\n",
    "                'embedding': embedding\n",
    "            })\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open('embedding_results.json', 'w') as outfile:\n",
    "    json.dump(results, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6492610-d0f1-497c-bf3c-cc5e53260fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
