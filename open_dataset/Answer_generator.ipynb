{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "import os\n",
    "from random import shuffle\n",
    "openai.api_key = \"\"\n",
    "questions_file_path = 'All-Questions.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_banks = {\n",
    "    \"mike_questions\": 'Mike-Questions.xlsx',\n",
    "    \"abram_questions\": 'Abram-Questions.xlsx',\n",
    "    \"haoran_questions\": 'Haoran-Questions.xlsx',\n",
    "    \"anurag_questions\": 'Anurag-Questions.xlsx',\n",
    "    \"siyu_questions\": 'Siyu-Questions.xlsx',\n",
    "    \"ziwei_questions\": 'Ziwei-Questions.xlsx',\n",
    "    # \"ryan_questions\": 'Ryan-Questions.xlsx',\n",
    "    # \"other_questions\": 'Other-Questions.xlsx',\n",
    "}\n",
    "\n",
    "########### each question id is encoded by the following format: ##########\n",
    "# 0 place:   difficulty (easy, medium, hard)\n",
    "# 1 place:   type (known, paraphrase, original)\n",
    "# 2 place:   context (no context, relevant, vague, irrelevant)\n",
    "# 3 place:   author\n",
    "# 4-6 place: question number\n",
    "###########################################################################\n",
    "\n",
    "difficulty_encoder = {\n",
    "    \"easy\": '1',\n",
    "    \"Easy\": '1',\n",
    "    \"medium\": '2',\n",
    "    \"Medium\": '2',\n",
    "    \"hard\": '3',\n",
    "    \"Hard\": '3',\n",
    "}\n",
    "\n",
    "type_encoder = {\n",
    "    \"Known\": '0',\n",
    "    \"Copied\": '0',\n",
    "    \"Paraphrase\": '1',\n",
    "    \"original\": '2',\n",
    "    \"Original\": '2',\n",
    "}\n",
    "\n",
    "context_encoder = {\n",
    "    \"No Context\": '0',\n",
    "    \"Relevant\": '1',\n",
    "    \"Vague\": '2',\n",
    "    \"Irrelevant\": '3',\n",
    "}\n",
    "\n",
    "author_encoder = {\n",
    "    \"mike_questions\": '0',\n",
    "    \"abram_questions\": '1',\n",
    "    \"haoran_questions\": '2',\n",
    "    \"anurag_questions\": '3',\n",
    "    \"siyu_questions\": '4',\n",
    "    \"ziwei_questions\": '5',\n",
    "    \"ryan_questions\": '6',\n",
    "    \"other_questions\": '7',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/4ql4554d38z0mng49tjt76q40000gn/T/ipykernel_38580/340960892.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2000000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n",
      "/var/folders/qn/4ql4554d38z0mng49tjt76q40000gn/T/ipykernel_38580/340960892.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1001000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n",
      "/var/folders/qn/4ql4554d38z0mng49tjt76q40000gn/T/ipykernel_38580/340960892.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1002000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n",
      "/var/folders/qn/4ql4554d38z0mng49tjt76q40000gn/T/ipykernel_38580/340960892.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1003000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n",
      "/var/folders/qn/4ql4554d38z0mng49tjt76q40000gn/T/ipykernel_38580/340960892.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '3204000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n",
      "/var/folders/qn/4ql4554d38z0mng49tjt76q40000gn/T/ipykernel_38580/340960892.py:11: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1005000' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n"
     ]
    }
   ],
   "source": [
    "def merge_all_questions(questions_banks):\n",
    "    all_questions = pd.DataFrame()\n",
    "    for bank in question_banks:\n",
    "        bank_df = pd.read_excel(question_banks[bank])\n",
    "        if 'Number' in bank_df.columns:\n",
    "            bank_df.drop(columns=['Number'], inplace=True)\n",
    "        for idx in range(len(bank_df)):\n",
    "            difficulty = bank_df.at[idx, 'Difficulty']\n",
    "            question_type = bank_df.at[idx, 'Type']\n",
    "            prefix = difficulty_encoder[difficulty]+type_encoder[question_type]+'0'+author_encoder[bank]\n",
    "            bank_df.at[idx, 'ID'] = str(prefix + format(idx, '03d'))\n",
    "        # Move the ID column to the first position\n",
    "        cols = ['ID'] + [col for col in bank_df.columns if col != 'ID']\n",
    "        bank_df = bank_df[cols]\n",
    "        all_questions = pd.concat([all_questions, bank_df])\n",
    "    all_questions = all_questions.reset_index(drop=True)\n",
    "    all_questions.to_excel(questions_file_path, index=False)\n",
    "\n",
    "merge_all_questions(question_banks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excel_to_json(file_path):\n",
    "    json_file_path = 'Question_bank.json'\n",
    "    df = pd.read_excel(file_path)\n",
    "    df.rename(columns={'Question':'question','Ground Truth':'answer','Relevant Context':'relevant','Vague Context':'vague','Irrelevant Context':'irrelevant'}, inplace=True)\n",
    "    \n",
    "    # take only questions, ground truth, and context\n",
    "    df = df[['ID','question','answer','relevant','vague','irrelevant']]\n",
    "\n",
    "    # save to json\n",
    "    data = df.to_json(orient='records')\n",
    "    df.to_json(json_file_path,orient='records')\n",
    "    return json.loads(data)\n",
    "\n",
    "extracted_data = convert_excel_to_json(questions_file_path)\n",
    "shuffle(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query GPT for Answering Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_generate_answer(id, question,generated_answer,ground_truth):\n",
    "    text = f\"Question:{question}\\n\\n\"\n",
    "    text += f\"<br><br>\\n<strong>Generated Answer: </strong>{generated_answer}\\n\\n\"\n",
    "    text += f\"<br><br>\\n<strong>Ground Truth: </strong>{ground_truth}\\n\\n\"\n",
    "    text += f\"<br>\\n<hr size='2'>\\n\"\n",
    "    text += f\"<strong>How good is the generated answer?</strong>\"\n",
    "    answer = {\n",
    "        'id': id,\n",
    "        'question': 'How good is the generated answer?',\n",
    "        'text':text\n",
    "    }\n",
    "    return answer\n",
    "\n",
    "def generate_answer(extracted_data,json_file_path):\n",
    "    generated_answers = []\n",
    "    raw_generated_answers = []\n",
    "    for idx,item in enumerate(extracted_data):\n",
    "        messages = []\n",
    "        messages.append({\"role\": \"system\", \"content\": \"\"\"\n",
    "                        You are QuantumGPT, a tool that answers quantum related questions. Please answer the question and write equations in latex.\n",
    "                        \"\"\"})\n",
    "        # *************No context**********\n",
    "        message_nh=messages\n",
    "        message_nh.append({\"role\": \"user\", \"content\": item['question']})\n",
    "        response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4-1106-preview\",\n",
    "                    temperature=1.0,\n",
    "                    max_tokens=1000,\n",
    "                    messages=message_nh,\n",
    "                )\n",
    "        response_no_context=response.choices[0].message.content\n",
    "        id_no_context = id\n",
    "        answer_no_context = reformat_generate_answer(id_no_context,item['question'],response_no_context,item['answer'])\n",
    "        generated_answers.append(answer_no_context)\n",
    "        # print('No context generated')\n",
    "        time.sleep(60)\n",
    "\n",
    "        # *******Relevant context*****\n",
    "        message_h1=messages\n",
    "        message_h1.append({\"role\": \"assistant\", \"content\": item['relevant']})\n",
    "        message_h1.append({\"role\": \"user\", \"content\": item['question']})\n",
    "        response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4-1106-preview\",\n",
    "                    temperature=1.0,\n",
    "                    max_tokens=1000,\n",
    "                    messages=message_h1,\n",
    "                )\n",
    "        response_relevant =response.choices[0].message.content\n",
    "        id_relevant = list(id)\n",
    "        id_relevant[2] = context_encoder['Relevant']\n",
    "        id_relevant = ''.join(id_relevant)\n",
    "        answer_relevant = reformat_generate_answer(id_relevant,item['question'],response_relevant,item['answer'])\n",
    "        generated_answers.append(answer_relevant)\n",
    "        # print('Relevant generated')\n",
    "        time.sleep(60)\n",
    "\n",
    "        # *******Vague context********\n",
    "        message_h2=messages\n",
    "        message_h2.append({\"role\": \"assistant\", \"content\": item['vague']})\n",
    "        message_h2.append({\"role\": \"user\", \"content\": item['question']})\n",
    "        response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4-1106-preview\",\n",
    "                    temperature=1.0,\n",
    "                    max_tokens=1000,\n",
    "                    messages=message_h2,\n",
    "                )\n",
    "        response_vague=response.choices[0].message.content\n",
    "        id_vague = list(id)\n",
    "        id_vague[2] = context_encoder['Vague']\n",
    "        id_vague = ''.join(id_vague)\n",
    "        answer_vague = reformat_generate_answer(id_vague,item['question'],response_vague,item['answer'])\n",
    "        generated_answers.append(answer_vague)\n",
    "        # print('Vague generated')\n",
    "        time.sleep(60)\n",
    "\n",
    "        # **********Irrelevant context********\n",
    "        message_h3=messages\n",
    "        message_h3.append({\"role\": \"assistant\", \"content\": item['irrelevant']})\n",
    "        message_h3.append({\"role\": \"user\", \"content\": item['question']})\n",
    "        response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4-1106-preview\",\n",
    "                    temperature=1.0,\n",
    "                    max_tokens=1000,\n",
    "                    messages=message_h3,\n",
    "                )\n",
    "        response_irrelevant=response.choices[0].message.content\n",
    "        id_irrelevant = list(id)\n",
    "        id_irrelevant[2] = context_encoder['Irrelevant']\n",
    "        id_irrelevant = ''.join(id_irrelevant)\n",
    "        answer_irrelevant = reformat_generate_answer(id_irrelevant,item['question'],response_irrelevant,item['answer'])\n",
    "        generated_answers.append(answer_irrelevant)\n",
    "        # print('Irrelevant generated')\n",
    "        time.sleep(60)\n",
    "\n",
    "        answer = {\n",
    "            'question': item['question'],\n",
    "            'no_context': response_no_context,\n",
    "            'relevant': response_relevant,\n",
    "            'vague': response_vague,\n",
    "            'irrelevant': response_irrelevant,\n",
    "            'ground_truth': item['answer']\n",
    "        }\n",
    "        raw_generated_answers.append(answer)\n",
    "\n",
    "        with open(json_file_path, 'w') as f:\n",
    "            json.dump(generated_answers, f)\n",
    "        with open('Raw_Generated_answer_bank.json', 'w') as f:\n",
    "            json.dump(raw_generated_answers, f)\n",
    "    return generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000003\n",
      "No context generated\n",
      "Relevant generated\n",
      "Vague generated\n",
      "Irrelevant generated\n",
      "2204005\n",
      "No context generated\n",
      "Relevant generated\n",
      "Vague generated\n",
      "Irrelevant generated\n",
      "1000000\n",
      "No context generated\n",
      "Relevant generated\n",
      "Vague generated\n",
      "Irrelevant generated\n",
      "0005004\n",
      "No context generated\n",
      "Relevant generated\n",
      "Vague generated\n",
      "Irrelevant generated\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'Generated_answer_bank.json'\n",
    "generated_answers = generate_answer(extracted_data,json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bank = pd.read_json('Generated_answer_bank.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bank.to_excel('Generated_answer_bank.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(generate_bank)):\n",
    "#     id_int = generate_bank.at[idx, 'id']\n",
    "#     id_str = str(id_int)\n",
    "#     id_int += 1000000\n",
    "#     generate_bank.at[idx, 'id'] = id_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_bank.to_json('Corrected_Generated_answer_bank.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
